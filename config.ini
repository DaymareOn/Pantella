[Startup]
; Open the basic config editor when starting Mantella.exe by setting this to 1
;   Options: 0, 1
open_config_editor = 0

[Game]
; game_id - Choose the game to use with Mantella, options are: skyrim, fallout4
game_id = skyrim

[Paths]
; Directories used by Mantella
; skyrim_folder
; 	If you are using a Wabbajack modlist, Mod Organizer 2 may be storing your Skyrim folder in MO2\overwrite\Root 
; 	If that is the case, try setting this path as your skyrim_folder if pointing to your actual Skyrim folder doesn't work
;   If this path is incorrect, casting a spell on an NPC will will only end a conversation
;   default = C:\Games\Steam\steamapps\common\Skyrim Special Edition
skyrim_folder = C:\Games\Steam\steamapps\common\Skyrim Special Edition

; xvasynth_folder
;   The folder you have xVASynth downloaded to (the folder that contains xVASynth.exe)
;   default = C:\Games\Steam\steamapps\common\xVASynth
xvasynth_folder = C:\Games\Steam\steamapps\common\xVASynth

; mod_folder
;   This is the path to the Mantella spell
;   If you are using Mod Organizer 2, this path can be found by right-clicking the Mantella mod in your mod list
;   and selecting "Open in Explorer"
;   If you are using Vortex, this path needs to be set to your Skyrim\Data folder
;   eg C:\Games\Steam\steamapps\common\Skyrim Special Edition\Data
;   If this path is incorrect, NPCs will say the same voiceline on repeat
;   default = C:\Modding\MO2\mods\Mantella
mod_folder = C:\Modding\MO2\mods\Mantella

; Character data file/folder - Refers to either a single csv file that contains all character data or a folder that contains multiple json files that contain character data
character_database_file = ./data/skyrim_characters.csv

; voice model ref ids - Used to determine which voice model to use for unnamed NPCs
voice_model_ref_ids_file = ./voice_model_ids.json

; logging_file_path
;   The path to the log file
;   default = ./logging.log
logging_file_path = ./logging.log

; language_support_file_path
;   The path to the language support file
language_support_file_path = ./data/language_support.csv


[Language]
; language
; 	The language used by ChatGPT, xVASynth, and Whisper
; 	Options: en, ar, da, de, el, es, fi, fr, hu, it, ko, nl, pl, pt, ro, ru, sv, sw, uk, ha, tr, vi, yo
language = en

; end_conversation_keyword
;   The keyword Mantella will listen out for to end the conversation (you can also end conversations by re-casting the Mantella spell)
end_conversation_keyword = Goodbye

; goodbye_npc_response
;   The response the NPC gives at the end of the conversation
goodbye_npc_response = Safe travels

; collecting_thoughts_npc_response
;   The response the NPC gives when they need to summarise the conversation because the maximum token count has been reached
collecting_thoughts_npc_response = I need to gather my thoughts for a moment


[Microphone]
; microphone_enabled
;   Whether to use microphone input (1) or text input (0)
;   NOTE: This setting is overwritten by the MCM menu setting if that setting has been configured
;   Options: 0, 1
microphone_enabled = 1

; model_size
; 	The size of the Whisper model used. Some languages require larger models. The base.en model works well enough for English
; 	See here for a comparison of languages and their Whisper performance: 
; 	https://github.com/openai/whisper#available-models-and-languages
; 	Options: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, large-v2, or whisper-1 (if using OpenAI API, see whisper_type setting below)
model_size = base

; language
;   The user's spoken language
;   The two letter ISO 639-1 language code
;   default = The one set in [Language]language above
stt_language = default

; translate
;   Translate the transcribed speech to English if supported by the Speech-To-Text engine (only impacts faster_whisper option, no impact on whispercpp, which is controlled by your server)
;   STTs that support this function: Whisper (faster_whisper)
;   Options: 0, 1
stt_translate = 0

; process_device
;   Whether to run Whisper on your CPU or NVIDIA GPU (with CUDA installed) (only impacts faster_whisper option, no impact on whispercpp, which is controlled by your server)
; 	Options: cpu, cuda
process_device = cpu

; whisper_type
;   Advanced users only. Allows using whispercpp (https://github.com/ggerganov/whisper.cpp) in server mode instead of default faster_whisper.
;   Alternatively, can be used to run Whisper via the OpenAI API.
;   The main benefits would be to reduce vram usage when using larger whisper models, to enable use of distil-whisper models,
;   to share a whisper speech to text service between AI mods like Mantella and Herika, or run the whispercpp server in a cloud service.
;   In whispercpp server mode, the server settings, not the ones above, will control the model you use and cpu vs. gpu usage.  
;   You are expected to "bring your own server" and have whispercpp running while running Mantella.
;   If the default works for you, DO NOT change this variable. 
;   To change to whispercpp server mode / OpenAI API instead, enter whispercpp. 
;   Additionally, if using the OpenAI API, ensure your GPT_SECRET_KEY.txt is an OpenAI key, whisper_url is "https://api.openai.com/v1/audio/transcriptions" below, and model_size is "whisper-1" above
;   default: faster_whisper
whisper_type = faster_whisper

; whisper_url
;   Advanced users only. Allows entering a openai-compatible server url. If you use whispercpp above in whisper_type, then enter the whispercpp server URL here.
;   Note that if you are also using the Herika mod, the default 8080 port used by whispercpp server may conflict with Herika. You can change the port to, e.g., 8070 instead to avoid the conflict.
;   Examples: http://127.0.0.1:8080/inference (default) / http://127.0.0.1:8070/inference (if you use the optional --port 8070 comand line argument), https://api.openai.com/v1/audio/transcriptions (if using OpenAI API)
whisper_url = http://127.0.0.1:8080/inference

; audio_threshold
; 	Controls how much background noise is filtered out
; 	If the mic is not picking up speech, try lowering this value
; 	If the mic is picking up too much background noise, try increasing this value
; 	Set this value to auto to let the script decide (only recommended if you are trying to fix mic issues, otherwise this option can be inconsistent)
;   It is better to find the right fixed number for your mic in the long run
; 	Options: auto, 1-999
;   Recommended: 175
audio_threshold = auto

; pause_threshold
;   How long to wait (in seconds) before converting mic input to text
;   If you feel like you are being cut off before you finish your response, increase this value
;   If you feel like there is too much of a delay between you finishing your response and the text conversion, decrease this value
;   Minimum: 0.5
pause_threshold = 0.5

; listen_timeout
;   How long to wait (in seconds) for the player to speak before retrying
;   This needs to be set to ensure that Mantella can periodically check if the conversation has ended
;   Recommended: 30
listen_timeout = 30


[Hotkey]
; hotkey
;   The hotkey can be configured in Mantella's MCM menu

; textbox_timer
;   The textbox timer can be configured in Mantella's MCM menu


[LanguageModel]
; inference_engine - The backend used to run the LLM / connect to the LLM API
; Options: default, openai(default), llama-cpp-python
; openai: Uses the OpenAI API, either using the default openai API or an alternative API base (set in the config below)
inference_engine = default

; tokenizer_type - The tokenizer used to get the token count of text
; Options: default, tiktoken, embedding
; default: Uses the recommended tokenizer for the inference engine being used. Some LLMS come with their own tokenizer, so this option will use that tokenizer if it exists
; tiktoken: Fast but only works for specific models (Mainly OpenAI's models, but it should work for any model that uses the same tokenizer as OpenAI's models)
; embedding: ONLY WORKS WHEN USING openai INFERENCE ENGINE! Slow but good for compatibility with any model using the openai API
tokenizer_type = default

; maximum_local_tokens
;   If the model chosen is not recognised by Mantella, the token count for the given model will default to this number
;   If this is not the correct token count for your chosen model, you can change it here
;   Keep in mind that if this number is greater than the actual token count of the model, then Mantella will crash if a given conversation exceeds the model's token limit
;   Default: 4096
maximum_local_tokens = 4096

; max_response_sentences
; 	The maximum number of sentences returned by the LLM. Lower this value to reduce waffling
max_response_sentences = 999

; wait_time_buffer
;   Time to wait (in seconds) before generating the next voiceline
;   Mantella waits for the duration of a given voiceline's .wav file + an extra buffer to account for processing overhead within Skyrim
;   If you are noticing that some voicelines are not being said in-game, try increasing this buffer
;   Default: 1.0
wait_time_buffer = 2

; stop 
;   A list of strings that will be used to indicate the end of a conversation - Default for OpenAI API is #, but you may want to add more or remove # from the listif you are using a local model
;   Note: A list of up to FOUR strings can be used when using OpenAI API, can be as many as you want with any other inference engine. <3
;   If you want more than one stopping string use this format: string1,string2,string3,string4
;   By default only # is used, but you can add more if you are using a local model
stop = <im_end>,\n<im_end>

; The following parameters will be used as models support them. However, not all infernece engines support all sampling styles.
; temperature - Higher values will result in more random responses, lower values will result in more predictable responses
;   Decimal number between 0 and 2
;   Default: 1
temperature = 1
; top_p - Higher values will result in more random responses, lower values will result in more predictable responses
;   Decimal number between 0 and 1 - 1 is disabled
;   Default: 1
top_p = 1
; min_p  - Higher values will result in more random responses, lower values will result in more predictable responses
;   Decimal number between 0 and 1 - 1 is disabled
;   Default: 0.05
min_p = 0.05
; top_k - Archaic sampling method, but still works. Higher values will result in more random responses, lower values will result in more predictable responses.
;   Integer value - 0 is disabled
;   Default: 0
top_k = 0
; repeat_penalty - Penalizes the model for repeating itself. Higher values will result in less repetition, lower values will result in more repetition.
;   Decimal number between 1.0 and 2.5(can technically go higher I think?) - 1.0 means no penalty
;   default: 1.0 
repeat_penalty = 1.0
; max_tokens - The maximum number of tokens to be returned by the LLM before yielding to the player. Lowering this value can sometimes result in empty responses, but leaves more available context for the LLM to use, especially with inference engines that don't support a sliding context window (llama-cpp-python, etc).
;   Integer value
;   Lowering this value can sometimes result in empty responses
;   Default: 512
max_tokens = 512

; Set how messages are to be formatted for the LLM - Defaults work for OpenAI API, but may need to be changed for local models.
; This is the token that will be used to indicate the start of a conversation - Default for OpenAI API is <im_start>
BOS_token = <im_start>
; This is the token that will be used to indicate the end of a conversation - Default for OpenAI API is <im_end>
EOS_token = <im_end>
; This is placed between the nanme of the speaker and their response - Default for OpenAI API is \n
message_signifier = \n
; This is placed between each response - Default for OpenAI API is \n
message_seperator = \n
; Format the message to be sent to the LLM - Default for OpenAI API is [BOS_token][name][message_signifier][content][EOS_token][message_seperator]
; Do not change this unless you know what you are doing
; default: [BOS_token][name][message_signifier][content][EOS_token][message_seperator]
message_format = [BOS_token][name][message_signifier][content][EOS_token][message_seperator]

; Set the names of the system, user, and assistant for use in standalone prompts and conversation summaries - Defaults work for OpenAI API, but may need to be changed for local models.
; system_name - This is the name of the system used as the role for system messages - Default for OpenAI API is system
system_name = system
; user_name - This is the name of the internal user used for summarizing the conversation - Default for OpenAI API is user
user_name = user
; assistant_name - This is the name of the internal assistant used for summarizing the conversation - Default for OpenAI API is assistant
assistant_name = assistant

; assist_check - Whether to check the response and toss it out if it has the word "assist" in it. Apparently helpful for OpenAI, but incredibly annoying for local models.
;   Default is 1 (enabled), 0 for local models (disabled)
assist_check = 1
; strip_smalls - Whether to remove sentences that only have a few words in them.
;   Default is 1 (enabled), 0 to disabled
strip_smalls = 1
; small_size - The size of a small sentence. If a sentence is smaller than this, it will be skipped.
;   Default: 3
small_size = 3
; same_output_limit - The number of times the same output can be repeated before generation is restarted.
;   Default: 30
same_output_limit = 30


; conversation_limit_pct - The percentage of the available tokens after the prompt is appended and formatted that can be used before a conversation is reloaded - Default for OpenAI API is 0.45
conversation_limit_pct = 0.45

; reload_buffer - The number of messages to add back to the context after a conversation is reloaded and summarised to prevent the conversation from reaching the token limit, but also prevent NPCs from forgetting everything. This is a slightly lossy process, as it will abbreviate the last conversation into a single paragraph that sums up the ghist of what occured during this period of time. After it does this, the reload_buffer number of messages will be added back to the context from the previous ongoing conversation.
; default: 8
reload_buffer = 8

; reload_wait_time - How long after a reload to wait for the disk to write the new summary file before continuing.
; default: 1
reload_wait_time = 1


; experimental_features
;   Enable NPC actions based on LLM output:
;   - Offended: NPCs can attack you
;   - Forgiven: NPCs can end combat with you
;   - Follow: NPCs can agree to follow you (enables the "Follow me. I need your help." dialogue option)
;   These features are disabled by default due to some local models getting offended very easily and the lack of testing of the Follow command over long playthroughs
;   Please ensure to save frequently if enabling these features
experimental_features = 1

[openai_api]
; model
; 	Options: gpt-4, gpt-3.5-turbo, gpt-4-32k, gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, gpt-4-1106-preview
;   If using openrouter.ai, place here the name of the model you want to use with openrouter in openrouter's provided format in its documentation: https://openrouter.ai/docs#models. Example: meta-llama/llama-2-70b-chat
;   Default: gpt-3.5-turbo-1106
model = gpt-3.5-turbo-1106

; alternative_openai_api_base
;   If you are using openai's services, leave this alone, otherwise you can change this variable to another base_api that uses openai's api
;   For example, if you have a local llm framework or online framework that allows you to use a different url to access openai api functions, you can enter the base_api url here 
;   Your alternative api_base must support openai's python streaming protocol.
;   Examples: 
;       http://127.0.0.1:5000/v1 for textgenwebui using the default openai extension
;       http://127.0.0.1:5001/v1 for koboldcpp (after version 1.46 of koboldcpp which supports the openai API)
;       http://localhost:8080/v1 using the default endpoint for Local.ai
;       http://localhost:8000/v1 using llama-cpp-python[server]
;       https://openrouter.ai/api/v1 for openrouter
;       http://localhost:5001/v1 for using koboldcpp locally or the url you obtain from the koboldcpp google colab notebook with /v1 added at the end.
;   Ensure that you have the correct secret key set in GPT_SECRET_KEY.txt for the service you are using
;   Note that for some services, like textgenwebui, you must enable the openai extension and have the model you want to use preloaded before running mantella
;   Leave this value as none to use the normal openai chat gpt models.
alternative_openai_api_base = none

; secret_key_file_path
;   The path to your OpenAI secret key file (Only needed if using the OpenAI API, usually not needed for other models using an emulated API)
secret_key_file_path = ./GPT_SECRET_KEY.txt

[llama_cpp_python]
; model_path - Only used if inference_engine is set to llama-cpp-python
; The path to the model you want to use with llama-cpp-python when running the model locally and don't want to use the openai API
; If you are using the openai API, leave this value as none
model_path = none

; n_gpu_layers - The number of layers to run on the GPU when using llama-cpp-python
; Options: 0-99 (0 = all layers on CPU, 99 = all layers on GPU)
; Different model sizes have different numbers of layers, so you may need to experiment with this value to find the optimal setting for your model and GPU combination. More layers offloaded uses more VRAM and less DRAM(System RAM) but is faster. Fewer layers offloaded uses less VRAM and more DRAM but is slower. If there are any layers on your CPU, it will use a lot of CPU power based on the n_threads setting below.
; default: 0 (all layers on CPU) - This is the safest option, but also the slowest. If you have an NVIDIA GPU with over 8GB of VRAM, try setting this incrementally higher values until you crash when running out of VRAM. Then set it to the last value that worked.
n_gpu_layers = 0

; n_threads - The number of threads to use when running layers on the CPU when using llama-cpp-python
; Options: 0-however many threads your CPU has(probably like 4-8 is what you want if you have an 8-16 thread CPU, just set it to half of your total threads(or if you know how many cores you have, set it to that exact number. If you're running Intel, set it to number of performance cores  yuor CPU has, not the total number of cores. If you're running AMD, set it to the total number of cores your CPU has.))
; default: 4 (half of an 8 thread CPU, which is pretty common) Set lower or higher depending on your hardware if you're unhappy with performance.
n_threads = 4

; n_batch - Maximum number of prompt tokens to batch together when calling llama_eval. Higher values use more VRAM but are faster. Lower values use less VRAM but are slower. If you have a lot of VRAM, try setting this to 4096 or 8192. If you have less VRAM, try setting this to 2048 or 1024. If you have a lot of VRAM and a fast CPU, you can try setting this to 1024. If not, you should probably leave it at 512.
; Options: 1-8192
; default: 512
n_batch = 512

; tensor_split - If you have more than one CUDA enabled GPU in your computer, you can use this to decide the ratio of which GPU gets how much of the model. Split values by commas, must by floats between 0 and 1. (0.0-1.0)
; default: 1.0 (all on GPU 0) If you have two GPUS and you want it all on the second GPU, use: 0.0,1.0 And if you want it split evenly between the two GPUs, use: 0.5,0.5
tensor_split = 1.0

; main_gpu - If you have more than one CUDA enabled GPU in your computer, you can use this to decide which GPU to use for the main model. 0 is the first GPU, 1 is the second GPU, etc.
; If your tensor_split is 0.0, 1.0, then you should set this to 1. If your tensor_split is 0.5, 0.5, then you should set this to whichever GPU is faster between the two. Otherwise leave as is.
; default: 0
main_gpu = 0


[Speech]
; tts_process_device
;   Whether to run xVASynth server (unless already running) on your CPU or a NVIDIA GPU (with CUDA installed)
;   Options: cpu, gpu
tts_process_device = cpu

; pace
;   The default speed of talking. Also varies between voices.
;       0.5 = 2x faster; 2 = 2x slower
;   Options: 0.1-2
;   Recommended: 1.0
;   Note that at the time of writing, this setting does not work with xVASynth v3.0.3 or less, but may work with future releases
pace = 1.0

; use_cleanup
;   Whether to try to reduce noise and the robot-sounding nature of xVASynth generated speech. Has only slight impact on processing speed for the CPU. Not meant to be used on voices that have post-effects attached to them (echoes, reverbs, etc.)
;   Options: 0, 1
use_cleanup = 0

; use_sr
;   Whenever to improve the quality of your audio through Super-resolution of 22050Hz audio into 48000Hz audio. Keep the Hz setting within xVASynth to something higher like 48000 or 44100. Also to note, this is a fairly slow process on the CPU, but on some GPUs, it can be quick.
;   Options: 0, 1
;   Recommended: 0
use_sr = 0

; xvasynth_base_url
;    Internal IP address of the xVASynth server. Only change if you are hosting xVASynth on a different computer and want to use it with Mantella. (NOTE: If you are using xVASynth on another computer, please make sure to run it in headless mode or the required endpoints will not be available)
xvasynth_base_url = http://127.0.0.1:8008

; post conversation wait time
end_conversation_wait_time = 5

; sentences_per_voiceline - The number of sentences to be sent to the TTS engine at once. Note: The higher this value, the longer it will take for the TTS engine to start speaking. If the NPC doesn't respond as many sentences as this is set to, they will send their entire response at once.
;   Options: 1-999
;   Recommended: 1-3
;   Default: 2
sentences_per_voiceline = 2


[HUD]
; subtitles
;   Subtitles can be enabled via the "SETTINGS -> Display -> General Subtitles" option in Skyrim's menu


[Cleanup]
; remove_mei_folders
;   Clean up older instances of Mantella runtime folders from AppData/Local/Temp/_MEIxxxxxx
;   These folders build up over time when Mantella.exe is run
;   Enable this option to clean up these previous folders automatically when Mantella.exe is run
;   Disable this option if running this cleanup inteferes with other Python exes
;   For more details on what this is, see here: https://github.com/pyinstaller/pyinstaller/issues/2379
;   Options: 0, 1
remove_mei_folders = 0


[Debugging]
; debugging
; 	Whether debugging is enabled
; 	If this is set to 0, the values of all other variables in this section are ignored
; 	Options: 0, 1
debugging = 0

; play_audio_from_script
; 	Whether to play the generated voicelines directly from the script / exe
; 	Set this value to 1 if testing Mantella while Skyrim is not running
; 	Options: 0, 1
play_audio_from_script = 1

; debugging_npc
; 	Selects the NPC to test
; 	Set this value to None if you would instead prefer to select an NPC via the mod's spell
; 	Options: None, NPC name
debugging_npc = Hulda

; use_mic
; 	Whether the microphone is enabled
; 	When this value is set to 0, the sentence contained in default_player_response (see below) will be repeatedly sent to the LLM.
;   When this value is set to 1 and microphone_enabled is set to 0, allows you to write the response
; 	Options: 0, 1
use_mic = 0

; default_player_response
; 	The default text sent to the LLM if the microphone is not enabled
default_player_response = Can you tell me something about yourself?

; exit_on_first_exchange
; 	Whether to end the conversation after the first back and forth exchange
; 	Set this value to 1 if testing conversation saving on exit functionality
; 	Options: 0, 1
exit_on_first_exchange = 0

; add_voicelines_to_all_voice_folders
;   Whether to add all generated voicelines to all Skyrim voice folders
;   If you are experiencing issues with some NPCs not speaking, try setting this value to 1
;   Options: 0, 1
;   Recommended: 0
add_voicelines_to_all_voice_folders = 0


[Prompt]
;   The prompt is built using the below dynamic variables. These variables are replaced with the appropriate text when the prompt is sent to the LLM. You can edit the prompt below, but please ensure that the below dynamic variables are contained in curly brackets {} like you see below or the script will not be able to replace them with the appropriate text. If you make a bad prompt, you can expect performance to drop drastically, or complete gibberish to come out even. Only edit this if you know how to fix it if you break it. 
;   Dynamic variables that have an S to the left of them are Single-NPC replacements; They will be used if only one NPC is in the conversation, but not if more than one NPC is in the conversation.
;   Dynamic variables that have an M to the left of them are Multi-NPC replacements. They will be used if more than one NPC is in the conversation, but not if only one NPC is in the conversation.
; 		location = the current location
; 		time = the time of day as a number (eg 1, 22)
; 		time_group = the time of day in words (eg "in the morning", "at night")
; 		ampm = the time of day as AM or PM
; 		language = the selected language
; 		behavior_summary = The list of available behaviors for the NPC and the description of when to use them.
;       player_name = The real name of the playerconversations/NPC_Name/NPC_Name_summary_X.txt

; single_npc_prompt - The starting prompt sent to the LLM when an NPC is selected
;S 		name = the NPC's name
;S 		race = the NPC's race
;S 		gender = the NPC's gender
;S 		gendered_age = the NPC's gendered age
;S 		perspective_player_name = the player's name from the NPC's perspective
;S      perspective_player_description = The name of the player from the perspective of the character followed by their race, gender, and their trust level.
;S 		bio = the NPC's background description
;S 		trust = how well the NPC knows the player (eg "a stranger", "a friend")
;S 		conversation_summary = reads the latest conversation summaries for the NPC stored in data/conversations/NPC_Name/NPC_Name_summary_X.txt
single_npc_prompt = {name} is a {race} {gendered_age} that lives in Skyrim. {name} can only speak {language}.

    {bio}

    Sometimes in-game events will be sent as system messages with the text between * symbols. No one else can use these. Only System can use asterixes for providing context. Here is an example:

    *{player_name} picked up a pair of gloves*

    Here is another:
    
    *{name} dropped a Steel Sword*

    {name} is having a conversation with {player_name} in {location}.
    
    It is {time}:00{ampm} {time_group}.
    
    The following are Behaviors that {name} can use in addition to responding to {player_name}:
    {behavior_summary}
    The following is a summary of the conversation that {name} and {perspective_player_description} have had so far:
    {conversation_summary}
    The following is a conversation that will be spoken aloud between {name} and {perspective_player_description}. {name} will not respond with numbered lists, code, etc. only natural responses to the conversation.
; multi_npc_prompt - The starting prompt sent to the LLM when more than one NPC is selected
;M      names_w_player = The names of the NPCs in the conversation with the player's name at the end
;M      bios = The backgrounds for all the NPCs in the conversation
;M 		conversation_summaries = reads the latest conversation summaries for the NPCs stored in data/
;M      relationship_summary = The a list of perspective_player_descriptions with all the NPCs in the conversation separated by double newlines
multi_npc_prompt = {bios} 
    
    {conversation_summaries} 

    Sometimes in-game events will be sent as system messages with the text between * symbols. No one else can use these. Only System can use asterixes for providing context. Here is an example:

    *{player_name} picked up a pair of gloves*

    Here is another:
    
    *{player_name} dropped a Steel Sword*

    {names_w_player} are having a conversation in {location} in {language}.
    It is {time}:00{ampm} {time_group}.
